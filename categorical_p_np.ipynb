{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import pytz\n",
    "import collections\n",
    "import cntk as C \n",
    "from stop_words import get_stop_words\n",
    "from sklearn import preprocessing\n",
    "from email.parser import Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_emails(path):\n",
    "    # Reads emails from foulders where foulder is a category of emails in it\n",
    "    categories = os.listdir(path)\n",
    "    for c in categories:\n",
    "        if c[0] == '.':\n",
    "            categories.remove(c)\n",
    "    emails = []\n",
    "    for direc in categories:\n",
    "        files = os.listdir(path + \"/\" + direc)\n",
    "        f = []\n",
    "        for file in files:\n",
    "            file = open(path + \"/\" + direc + \"/\" + file)\n",
    "            parser = Parser()\n",
    "            email = parser.parse(file)\n",
    "            f.append(email)\n",
    "        emails.append(f)\n",
    "    return categories, emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relevant(emails, l):\n",
    "    # returns list of length l of relevant words that are \n",
    "    # the most common in the documents\n",
    "    words = []\n",
    "    for fold in emails:\n",
    "        for email in fold:\n",
    "            words += clear_text(email.get_payload())\n",
    "\n",
    "    counter = collections.Counter(words)\n",
    "    return [i[0] for i in counter.most_common()][:l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = get_stop_words('en')\n",
    "def clear_text(email):\n",
    "    # cleares text and returns lowercase relevant words\n",
    "    text = ''.join([i for i in email if i.isalpha() or i.isspace()])\n",
    "    text = re.split(r'[\\n \\t]+', text.lower())\n",
    "    text = [i for i in text if i not in stop and len(i) > 2]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_dataset(categories, folders, n):\n",
    "    # creates a vector from each document\n",
    "    words = relevant(folders, n)\n",
    "    print(\"Extracted relavent words\")\n",
    "    \n",
    "    data = []\n",
    "    for fold, cat in zip(folders, categories):\n",
    "        for email in fold:\n",
    "            email = clear_text(email.get_payload())\n",
    "            features = []\n",
    "            for i in range(n):\n",
    "                if words[i] in email:\n",
    "                    features.append(i)\n",
    "            data.append(features + [categories.index(cat)])\n",
    "        print(\"finished \" + cat)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the data file for CNTK reader\n",
    "def savetxt(filename, ndarray):\n",
    "    dir = os.path.dirname(filename)\n",
    "    \n",
    "    print(\"Saving\", filename )\n",
    "    with open(filename, 'w') as f:\n",
    "        labels = list(map(' '.join, np.eye(num_classes, dtype=np.uint).astype(str)))\n",
    "        for row in ndarray:\n",
    "            label_str = labels[row[-1]]\n",
    "            feature_str = ' '.join([str(i)+\":1\" for i in row[:-1]])\n",
    "            \n",
    "            line = '|labels {} |features {}\\n'.format(label_str, feature_str)\n",
    "            f.write(line)\n",
    "            #print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To read emails from folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories, folders = load_emails(\"enron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#num_classes = len(categories)\n",
    "num_classes = 17\n",
    "# also length of dictonary\n",
    "input_dim = 5106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted relavent words\n",
      "finished inbox\n",
      "finished contacts\n",
      "finished sent_items\n",
      "finished operations_committee_isas\n",
      "finished human_resources\n",
      "finished settlements\n",
      "finished preschedule\n",
      "finished symesees\n",
      "finished schedule_crawler\n",
      "finished el_paso\n",
      "finished personal\n",
      "finished bill_williams_iii\n",
      "finished calendar\n",
      "finished hr\n",
      "finished enron_messages\n",
      "finished rt_strat\n",
      "finished bill\n",
      "finished california_messages\n",
      "finished timbelden\n",
      "finished canada\n",
      "finished tie_meter_multipliers\n",
      "finished gwolfe\n",
      "finished rt_cuts\n",
      "finished forney\n"
     ]
    }
   ],
   "source": [
    "data = make_dataset(categories, folders, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving trainData/train.txt\n",
      "Saving trainData/test.txt\n"
     ]
    }
   ],
   "source": [
    "# save data for training and testing\n",
    "random.shuffle(data)\n",
    "n = len(data) // 10 * 8\n",
    "train, test = data[:n], data[n:]\n",
    "savetxt(\"trainData/train.txt\", train)\n",
    "savetxt(\"trainData/test.txt\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load actual data and split to train / test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testData = open(\"trainData/trainData.txt\")\n",
    "testData = testData.readlines()\n",
    "random.shuffle(testData)\n",
    "n = len(testData) // 10 * 8\n",
    "train, test = testData[:n], testData[n:]\n",
    "\n",
    "with open(\"trainData/train.txt\", 'w') as f:\n",
    "    for row in train:\n",
    "        f.write(row)\n",
    "\n",
    "with open(\"trainData/test.txt\", 'w') as f:\n",
    "    for row in test:\n",
    "        f.write(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read a CTF formatted text \n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        labels = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "        features   = C.io.StreamDef(field='features', shape=input_dim, is_sparse=True)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_hidden_layers = 2\n",
    "hidden_layers_dim = num_classes * 10\n",
    "\n",
    "input = C.input_variable(input_dim)\n",
    "label = C.input_variable(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(features):\n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.ops.relu):\n",
    "            h = features\n",
    "            for _ in range(num_hidden_layers):\n",
    "                h = C.layers.Dense(hidden_layers_dim)(h)\n",
    "            r = C.layers.Dense(num_classes, activation = None)(h)\n",
    "            return r\n",
    "        \n",
    "z = create_model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = C.cross_entropy_with_softmax(z, label)\n",
    "label_error = C.classification_error(z, label)\n",
    "\n",
    "learning_rate = 0.2\n",
    "lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "learner = C.sgd(z.parameters, lr_schedule)\n",
    "trainer = C.Trainer(z, (loss, label_error), [learner])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatch_size = 100\n",
    "num_samples_per_sweep = len(train)\n",
    "num_sweeps_to_train_with = 10\n",
    "num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "train_file = os.path.join(\"trainData/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0, Loss: 2.8176, Error: 95.00%\n",
      "Minibatch: 500, Loss: 0.1759, Error: 4.00%\n",
      "Minibatch: 1000, Loss: 0.1023, Error: 3.00%\n"
     ]
    }
   ],
   "source": [
    "# reader\n",
    "reader_train = create_reader(train_file, True, input_dim, num_classes)\n",
    "\n",
    "# input map\n",
    "input_map = {\n",
    "    label  : reader_train.streams.labels,\n",
    "    input  : reader_train.streams.features\n",
    "} \n",
    "\n",
    "# Run the trainer \n",
    "training_progress_output_freq = 500\n",
    "\n",
    "for i in range(0, int(num_minibatches_to_train)):\n",
    "    data = reader_train.next_minibatch(minibatch_size, input_map = input_map)\n",
    "    \n",
    "    trainer.train_minibatch(data)\n",
    "    batchsize, loss, error = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate on an unseen minibatch: 5.18%\n"
     ]
    }
   ],
   "source": [
    "test_file = os.path.join(\"trainData/test.txt\")\n",
    "reader_test = create_reader(test_file, False, input_dim, num_classes)\n",
    "input_map = {\n",
    "    label  : reader_test.streams.labels,\n",
    "    input  : reader_test.streams.features\n",
    "} \n",
    "\n",
    "data = reader_test.next_minibatch(1000000, input_map = input_map)\n",
    "error = trainer.test_minibatch(data)\n",
    "print('Error rate on an unseen minibatch: {:.2f}%'.format(error*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import pytz\n",
    "import collections\n",
    "import cntk as C \n",
    "from stop_words import get_stop_words\n",
    "from sklearn import preprocessing\n",
    "from email.parser import Parser\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_emails(path):\n",
    "    # Reads emails from foulders where foulder is a category of emails in it\n",
    "    categories = os.listdir(path)\n",
    "    for c in categories:\n",
    "        if c[0] == '.':\n",
    "            categories.remove(c)\n",
    "    emails = []\n",
    "    for direc in categories:\n",
    "        files = os.listdir(path + \"/\" + direc)\n",
    "        f = []\n",
    "        for file in files:\n",
    "            file = open(path + \"/\" + direc + \"/\" + file)\n",
    "            parser = Parser()\n",
    "            email = parser.parse(file)\n",
    "            f.append(email)\n",
    "        emails.append(f)\n",
    "    return categories, emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relevant(emails, l):\n",
    "    # returns list of length l of relevant words that are \n",
    "    # the most common in the documents\n",
    "    words = []\n",
    "    for fold in emails:\n",
    "        for email in fold:\n",
    "            words += clear_text(email.get_payload())\n",
    "\n",
    "    counter = collections.Counter(words)\n",
    "    return [i[0] for i in counter.most_common()][:l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = get_stop_words('en')\n",
    "def clear_text(email):\n",
    "    # cleares text and returns lowercase relevant words\n",
    "    text = ''.join([i for i in email if i.isalpha() or i.isspace()])\n",
    "    text = re.split(r'[\\n \\t]+', text.lower())\n",
    "    text = [i for i in text if i not in stop and len(i) > 2]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "universal = pytz.timezone (\"UTC\")\n",
    "beginning_of_times = datetime(2000, 1, 1, 0, 0, 0, 0, universal)\n",
    "\n",
    "def get_date(email):\n",
    "    # extracts date from email and converts it to a feature\n",
    "    date = datetime.strptime(email['Date'][:-6], '%a, %d %b %Y %H:%M:%S %z')\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_dataset(categories, folders, n):\n",
    "    # creates a vector from each document\n",
    "    words = relevant(folders, n)\n",
    "    print(\"Extracted relavent words\")\n",
    "    \n",
    "    data = []\n",
    "    for fold, cat in zip(folders, categories):\n",
    "        for email in fold:\n",
    "            date = get_date(email)\n",
    "            email = clear_text(email.get_payload())\n",
    "            features = []\n",
    "            for i in range(n):\n",
    "                if words[i] in email:\n",
    "                    features.append(i)\n",
    "            data.append(features + [date, categories.index(cat)])\n",
    "            \n",
    "        print(\"finished \" + cat)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the data file for CNTK reader\n",
    "def savetxt(filename, ndarray):\n",
    "    dir = os.path.dirname(filename)\n",
    "    \n",
    "    print(\"Saving\", filename )\n",
    "    with open(filename, 'w') as f:\n",
    "        labels = list(map(' '.join, np.eye(num_classes, dtype=np.uint).astype(str)))\n",
    "        for row in ndarray:\n",
    "            label_str = labels[row[-1]]\n",
    "           # date = row[-2]\n",
    "            date = get_time_feat(row[-2])\n",
    "            feature_str = ' '.join([str(i)+\":1\" for i in row[:-2]] + \\\n",
    "                                   [str(input_dim + i + 1) + \":\" + str(date[i]) for i in range(len(date))])\n",
    "            \n",
    "            line = '|labels {} |features {}\\n'.format(label_str, feature_str)\n",
    "            f.write(line)\n",
    "            #print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_feat(date):\n",
    "    date_f = []\n",
    "   # delta = beginning_of_times-date.astimezone(universal)\n",
    "   # date_f += [delta.total_seconds()]\n",
    "   # date_f += [delta.days]\n",
    "   # date_f += [date.year]\n",
    "   # date_f += [date.month]\n",
    "   # date_f += [date.isoweekday()]\n",
    "    date_f += [date.day]\n",
    "    return date_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To read emails from folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories, folders = load_emails(\"enron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = len(categories)\n",
    "#num_classes = 17\n",
    "# also length of dictonary\n",
    "#input_dim = 5106\n",
    "input_dim = 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted relavent words\n",
      "finished inbox\n",
      "finished contacts\n",
      "finished sent_items\n",
      "finished operations_committee_isas\n",
      "finished human_resources\n",
      "finished settlements\n",
      "finished preschedule\n",
      "finished symesees\n",
      "finished schedule_crawler\n",
      "finished el_paso\n",
      "finished personal\n",
      "finished bill_williams_iii\n",
      "finished calendar\n",
      "finished hr\n",
      "finished enron_messages\n",
      "finished rt_strat\n",
      "finished bill\n",
      "finished california_messages\n",
      "finished timbelden\n",
      "finished canada\n",
      "finished tie_meter_multipliers\n",
      "finished gwolfe\n",
      "finished rt_cuts\n",
      "finished forney\n"
     ]
    }
   ],
   "source": [
    "data = make_dataset(categories, folders, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving trainData/train.txt\n",
      "Saving trainData/test.txt\n"
     ]
    }
   ],
   "source": [
    "# save data for training and testing\n",
    "random.shuffle(data)\n",
    "n = len(data) // 10 * 8\n",
    "train, test = data[:n], data[n:]\n",
    "savetxt(\"trainData/train.txt\", train)\n",
    "savetxt(\"trainData/test.txt\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim += 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load actual data and split to train / test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testData = open(\"trainData/trainData.txt\")\n",
    "testData = testData.readlines()\n",
    "random.shuffle(testData)\n",
    "n = len(testData) // 10 * 8\n",
    "train, test = testData[:n], testData[n:]\n",
    "\n",
    "with open(\"trainData/train.txt\", 'w') as f:\n",
    "    for row in train:\n",
    "        f.write(row)\n",
    "\n",
    "with open(\"trainData/test.txt\", 'w') as f:\n",
    "    for row in test:\n",
    "        f.write(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read a CTF formatted text \n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        labels = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "        features   = C.io.StreamDef(field='features', shape=input_dim, is_sparse=True)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_hidden_layers = 2\n",
    "hidden_layers_dim = num_classes * 10\n",
    "\n",
    "input = C.input_variable(input_dim)\n",
    "label = C.input_variable(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(features):\n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.ops.relu):\n",
    "            h = features\n",
    "            for _ in range(num_hidden_layers):\n",
    "                h = C.layers.Dense(hidden_layers_dim)(h)\n",
    "            r = C.layers.Dense(num_classes, activation = None)(h)\n",
    "            return r\n",
    "        \n",
    "z = create_model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = C.cross_entropy_with_softmax(z, label)\n",
    "label_error = C.classification_error(z, label)\n",
    "\n",
    "learning_rate = 0.2\n",
    "lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "learner = C.sgd(z.parameters, lr_schedule)\n",
    "trainer = C.Trainer(z, (loss, label_error), [learner])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatch_size = 100\n",
    "num_samples_per_sweep = 5000#len(train)\n",
    "num_sweeps_to_train_with = 100\n",
    "num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "train_file = os.path.join(\"trainData/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0, Loss: nan, Error: 95.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-a1157ea60e0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_training_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_progress_output_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vlad/anaconda3/lib/python3.5/site-packages/cntk/train/trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, arguments, outputs, device)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontains_minibatch_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 updated = super(Trainer, self).train_minibatch_overload_for_minibatchdata(\n\u001b[0;32m--> 170\u001b[0;31m                     arguments, device)\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments,\n",
      "\u001b[0;32m/home/vlad/anaconda3/lib/python3.5/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36mtrain_minibatch_overload_for_minibatchdata\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch_overload_for_minibatchdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2500\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer_train_minibatch_overload_for_minibatchdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reader\n",
    "reader_train = create_reader(train_file, True, input_dim, num_classes)\n",
    "\n",
    "# input map\n",
    "input_map = {\n",
    "    label  : reader_train.streams.labels,\n",
    "    input  : reader_train.streams.features\n",
    "} \n",
    "\n",
    "# Run the trainer \n",
    "training_progress_output_freq = 500\n",
    "\n",
    "for i in range(0, int(num_minibatches_to_train)):\n",
    "    data = reader_train.next_minibatch(minibatch_size, input_map = input_map)\n",
    "    \n",
    "    trainer.train_minibatch(data)\n",
    "    batchsize, loss, error = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate on an unseen minibatch: 56.83%\n"
     ]
    }
   ],
   "source": [
    "test_file = os.path.join(\"trainData/test.txt\")\n",
    "reader_test = create_reader(test_file, False, input_dim, num_classes)\n",
    "input_map = {\n",
    "    label  : reader_test.streams.labels,\n",
    "    input  : reader_test.streams.features\n",
    "} \n",
    "\n",
    "data = reader_test.next_minibatch(1000000, input_map = input_map)\n",
    "error = trainer.test_minibatch(data)\n",
    "print('Error rate on an unseen minibatch: {:.2f}%'.format(error*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

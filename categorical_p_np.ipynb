{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import pytz\n",
    "import collections\n",
    "import cntk as C \n",
    "from stop_words import get_stop_words\n",
    "from sklearn import preprocessing\n",
    "from email.parser import Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_emails(path):\n",
    "    # Reads emails from foulders where foulder is a category of emails in it\n",
    "    categories = os.listdir(path)\n",
    "    for c in categories:\n",
    "        if c[0] == '.':\n",
    "            categories.remove(c)\n",
    "    emails = []\n",
    "    for direc in categories:\n",
    "        files = os.listdir(path + \"/\" + direc)\n",
    "        f = []\n",
    "        for file in files:\n",
    "            file = open(path + \"/\" + direc + \"/\" + file)\n",
    "            parser = Parser()\n",
    "            email = parser.parse(file)\n",
    "            f.append(email)\n",
    "        emails.append(f)\n",
    "    return categories, emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relevant(emails, l):\n",
    "    # returns list of length l of relevant words that are \n",
    "    # the most common in the documents\n",
    "    words = []\n",
    "    for fold in emails:\n",
    "        for email in fold:\n",
    "            words += clear_text(email.get_payload())\n",
    "\n",
    "    counter = collections.Counter(words)\n",
    "    return [i[0] for i in counter.most_common()][:l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = get_stop_words('en')\n",
    "def clear_text(email):\n",
    "    # cleares text and returns lowercase relevant words\n",
    "    text = ''.join([i for i in email if i.isalpha() or i.isspace()])\n",
    "    text = re.split(r'[\\n \\t]+', text.lower())\n",
    "    text = [i for i in text if i not in stop and len(i) > 2]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_dataset(categories, folders, n):\n",
    "    # creates a vector from each document\n",
    "    words = relevant(folders, n)\n",
    "    print(\"Extracted relavent words\")\n",
    "    \n",
    "    data = []\n",
    "    for fold, cat in zip(folders, categories):\n",
    "        for email in fold:\n",
    "            email = clear_text(email.get_payload())\n",
    "            features = []\n",
    "            for i in range(n):\n",
    "                if words[i] in email:\n",
    "                    features.append(i)\n",
    "            data.append(features + [categories.index(cat)])\n",
    "        print(\"finished \" + cat)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the data file for CNTK reader\n",
    "def savetxt(filename, ndarray):\n",
    "    dir = os.path.dirname(filename)\n",
    "    \n",
    "    print(\"Saving\", filename )\n",
    "    with open(filename, 'w') as f:\n",
    "        labels = list(map(' '.join, np.eye(num_classes, dtype=np.uint).astype(str)))\n",
    "        for row in ndarray:\n",
    "            label_str = labels[row[-1]]\n",
    "            feature_str = ' '.join([str(i)+\":1\" for i in row[:-1]])\n",
    "            \n",
    "            line = '|labels {} |features {}\\n'.format(label_str, feature_str)\n",
    "            f.write(line)\n",
    "            #print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories, folders = load_emails(\"enron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 18#len(categories)\n",
    "# also length of dictonary\n",
    "input_dim = 5106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted relavent words\n",
      "finished inbox\n",
      "finished contacts\n",
      "finished sent_items\n",
      "finished operations_committee_isas\n",
      "finished human_resources\n",
      "finished settlements\n",
      "finished preschedule\n",
      "finished symesees\n",
      "finished schedule_crawler\n",
      "finished el_paso\n",
      "finished personal\n",
      "finished bill_williams_iii\n",
      "finished calendar\n",
      "finished hr\n",
      "finished enron_messages\n",
      "finished rt_strat\n",
      "finished bill\n",
      "finished california_messages\n",
      "finished timbelden\n",
      "finished canada\n",
      "finished tie_meter_multipliers\n",
      "finished gwolfe\n",
      "finished rt_cuts\n",
      "finished forney\n"
     ]
    }
   ],
   "source": [
    "data = make_dataset(categories, folders, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data_p_np\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not Function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-6a42fced4743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_p_np\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-7d63990cdb74>\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(filename, ndarray)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mlabel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mfeature_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\":1\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not Function"
     ]
    }
   ],
   "source": [
    "savetxt(\"data_p_np\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testData = open(\"trainData/trainData.txt\")\n",
    "testData = testData.readlines()\n",
    "random.shuffle(testData)\n",
    "n = len(testData)//10 * 8\n",
    "train, test = testData[:n], testData[n:]\n",
    "\n",
    "with open(\"trainData/train.txt\", 'w') as f:\n",
    "    for row in train:\n",
    "        f.write(row)\n",
    "\n",
    "with open(\"trainData/test.txt\", 'w') as f:\n",
    "    for row in test:\n",
    "        f.write(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read a CTF formatted text \n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        labels = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "        features   = C.io.StreamDef(field='features', shape=input_dim, is_sparse=True)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_hidden_layers = 2\n",
    "hidden_layers_dim = 500#num_classes * 50\n",
    "\n",
    "input = C.input_variable(input_dim)\n",
    "label = C.input_variable(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(features):\n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.ops.relu):\n",
    "            h = features\n",
    "            for _ in range(num_hidden_layers):\n",
    "                h = C.layers.Dense(hidden_layers_dim)(h)\n",
    "            r = C.layers.Dense(num_classes, activation = None)(h)\n",
    "            return r\n",
    "        \n",
    "z = create_model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = C.cross_entropy_with_softmax(z, label)\n",
    "label_error = C.classification_error(z, label)\n",
    "\n",
    "learning_rate = 0.2\n",
    "lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "learner = C.sgd(z.parameters, lr_schedule)\n",
    "trainer = C.Trainer(z, (loss, label_error), [learner])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minibatch_size = 100\n",
    "num_samples_per_sweep = 60000\n",
    "num_sweeps_to_train_with = 4\n",
    "num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "#train_file = os.path.join(\"data_p_np\")\n",
    "train_file = os.path.join(\"trainData/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reader\n",
    "reader_train = create_reader(train_file, True, input_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0, Loss: 2.9182, Error: 97.00%\n",
      "Minibatch: 500, Loss: 0.1716, Error: 4.00%\n",
      "Minibatch: 1000, Loss: 0.0853, Error: 4.00%\n"
     ]
    }
   ],
   "source": [
    "# input map\n",
    "input_map = {\n",
    "    label  : reader_train.streams.labels,\n",
    "    input  : reader_train.streams.features\n",
    "} \n",
    "\n",
    "# Run the trainer \n",
    "training_progress_output_freq = 500\n",
    "\n",
    "for i in range(0, int(num_minibatches_to_train)):\n",
    "    data = reader_train.next_minibatch(minibatch_size, input_map = input_map)\n",
    "    \n",
    "    trainer.train_minibatch(data)\n",
    "    batchsize, loss, error = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_file = os.path.join(\"trainData/test.txt\")\n",
    "reader_test = create_reader(test_file, False, input_dim, num_classes)\n",
    "input_map = {\n",
    "    label  : reader_test.streams.labels,\n",
    "    input  : reader_test.streams.features\n",
    "} \n",
    "\n",
    "data = reader_test.next_minibatch(1000000, input_map = input_map)\n",
    "error = trainer.test_minibatch(data)\n",
    "print('Error rate on an unseen minibatch: {:.2f}%'.format(error*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels([18])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2913, 11632)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test), len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5103"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = []\n",
    "for f in test[:500]:\n",
    "    features = [i for i in f.split('|')[2].split(' ') if len(i) > 3]\n",
    "    l = [int(i.split(':')[0]) for i in features if len(i) < 8]\n",
    "    l.sort()\n",
    "    m.append(max(l))\n",
    "max(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5105:1\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
